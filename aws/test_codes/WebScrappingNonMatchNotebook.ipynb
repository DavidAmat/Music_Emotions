{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scrapping: Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "#Selenium options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"window-size=1400,1500\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"enable-automation\")\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "\n",
    "# Database\n",
    "import sqlalchemy as db\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re #for avoiding looking at titles with starting parenthesis\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "#S3 interaction\n",
    "from io import StringIO \n",
    "import boto3\n",
    "\n",
    "# Logging\n",
    "from v_log import VLogger\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [2020-03-31 17:23:44] Batch 1              | 1. Select all songs in nonmatch-query (S3)\n",
      "INFO     [2020-03-31 17:23:44] Batch 1              | 1. Select all songs in nonmatch-query (S3) (Completed)\n"
     ]
    }
   ],
   "source": [
    "path_local_log_file = f\"log/{batch_num}.log\"\n",
    "log = VLogger(f'Batch {batch_num}', uri_log = path_local_log_file, file_log_level = logging.INFO)\n",
    "\n",
    " \n",
    "\n",
    "#Regular expression to avoid strings in MSD initiated with NOT letters\n",
    "prog = re.compile(\"^[A-Za-z]\") \n",
    "\n",
    "def fun_clean_title(titart, prog, list_return = False):\n",
    "    \"\"\"\n",
    "    Cleans the title of a MSD song to avoid () or [] or any special character\n",
    "    \"\"\"\n",
    "    # CLEANING TITLE\n",
    "    words_list = titart.split(\" \")\n",
    "    # we want to avoid words that don't start with a character\n",
    "    words_set = set()\n",
    "    words_clean = list()\n",
    "    for ww in words_list:\n",
    "        result = prog.match(ww)\n",
    "        if result is not None: # avoid starting word with parenthesis\n",
    "            if '\\\\' not in ww: #avoid non-coded characters \\x19\n",
    "                if (\")\" not in ww) and (\"(\" not in ww):\n",
    "                    words_set.add(ww.lower())\n",
    "                    words_clean.append(ww.lower())\n",
    "    if not list_return:\n",
    "        return words_set\n",
    "    else:\n",
    "        return \" \".join(words_clean)\n",
    "\n",
    "    \n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "        # S3: functions\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "# S3 load df from batch file\n",
    "def load_df_s3(folder_path, file_name, S3_BUCKET = 'musicemotions'):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    path_S3 = folder_path + \"/\" + file_name  \n",
    "    csv_obj = s3.get_object(Bucket = S3_BUCKET,  Key = path_S3)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_string))\n",
    "    return df\n",
    "\n",
    "#save any local file in the EC2 instance to S3 \n",
    "def file_to_S3(local_path, S3_path,  S3_BUCKET = 'musicemotions'):\n",
    "    \"\"\"\n",
    "    local_path = os.path.join(\"..\",\"webscrapping\",\"log\",\"WebScrap.log\")\n",
    "    S3_path = nonmatch-query/log.txt\n",
    "    \"\"\"\n",
    "    if S3_path:\n",
    "        s3 = boto3.resource('s3')\n",
    "        resp = s3.Object(S3_BUCKET, S3_path).put(Body=open(local_path, 'rb'))\n",
    "    else:\n",
    "        s3 = boto3.resource('s3')\n",
    "        resp = s3.Object(S3_BUCKET, S3_path).put(Body=open(local_path, 'rb'))\n",
    "    return resp\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# S3: Look at nonmatch-query to find the query to launch webscrapping\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "folder_path = \"nonmatch-query\"\n",
    "name_file = f'{batch_num}.csv'\n",
    "\n",
    "# LOAD DATAFRAME FROM THE QUERY NONMATCH .CSV\n",
    "df = load_df_s3(folder_path, name_file)\n",
    "\n",
    "# Load dataframe of that batch\n",
    "log.info(\"1. Select all songs in nonmatch-query (S3)\")\n",
    "\n",
    "# Separate all the non alphabetical chracters with spaces, remove them and create a set of words\n",
    "df[\"query_clean_list\"] = df[\"query\"].apply(lambda x: fun_clean_title(x, prog, list_return=True))\n",
    "df[\"query_clean_list\"] = df[\"query_clean_list\"].apply(lambda x: re.sub('[^0-9a-zA-Z]+', ' ', x))\n",
    "df[\"query_clean_set\"] =  df[\"query_clean_list\"].apply(lambda x: set(x.split(\" \")))\n",
    "df  = df.sort_values([\"track_id\"])\n",
    "\n",
    "#Log\n",
    "log.info(\"1. Select all songs in nonmatch-query (S3) (Completed)\")\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#    WebScrapping - Functions\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# Query the non-matched titles\n",
    "\n",
    "# ### Functions\n",
    "\n",
    "def query_yt_song(qq_song, query_set):\n",
    "        #Search that artist on youtube\n",
    "        browser.get(f\"https://www.youtube.com/results?search_query={qq_song}\")\n",
    "        \n",
    "        # List all the elements in video-title\n",
    "        vid_title_elems = browser.find_elements_by_id('video-title')\n",
    "\n",
    "        # Save videos and their URL as tuple\n",
    "        for vte in vid_title_elems:\n",
    "            yt_title =  vte.get_attribute(\"title\")\n",
    "            yt_href  =  vte.get_attribute('href')\n",
    "            \n",
    "            #Compare that title with the query and if coincides in all words except 1 get that href\n",
    "            if compare_song_vs_title(yt_title, query_set):\n",
    "                #Make sure that the href is not a playlist (hence playlist does not have href: None)\n",
    "                if yt_href:\n",
    "                    return yt_href\n",
    "        return \"\"\n",
    "\n",
    "def compare_song_vs_title(yt_tit, query_set):\n",
    "    \n",
    "    # YOUTUBE SONG to SET (cleaned)\n",
    "    yt_set = fun_clean_title(yt_tit, prog, list_return=True) #returns a string\n",
    "    \n",
    "    #SUBSTITUTE ANY NON ALPHANUMERICA CHARACTERS by white space\n",
    "    yt_set = re.sub('[^0-9a-zA-Z]+', ' ', yt_set)\n",
    "    yt_set = set(yt_set.split(\" \")) #convert the words separated by spaces into a set\n",
    "    \n",
    "    #Maybe that set contains words with one letter, so in that case we will remove them\n",
    "    neat_query_set = set()\n",
    "    for nn in list(query_set):\n",
    "        if len(nn) > 1:\n",
    "            neat_query_set.add(nn)\n",
    "    query_set = neat_query_set; # only take the query set as the neat set without single letters or white spaces\n",
    "    \n",
    "    # Intersection\n",
    "    int_set = query_set.intersection(yt_set)\n",
    "    \n",
    "    # Compare the length of the yt_set with the query set\n",
    "    if len(int_set) >= (len(query_set) - 1): # allow one missing word in the intersection compared to the query set\n",
    "        return True\n",
    "    elif len(query_set) > 4: # if the query is bigger thatn 4, allow a intersection set coincidence of 2 words less\n",
    "        if len(int_set) >= (len(query_set) - 2):\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def load_df_s3(folder_path, file_name, S3_BUCKET = 'musicemotions'):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    path_S3 = folder_path + \"/\" + file_name  \n",
    "    csv_obj = s3.get_object(Bucket = S3_BUCKET,  Key = path_S3)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_string))\n",
    "    return df\n",
    "\n",
    "def save_df_to_S3(df, folder_path, name_file, S3_BUCKET = 'musicemotions'):\n",
    "    #Connect to S3\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    #Set the destination path\n",
    "    path_S3 = folder_path + \"/\" + name_file\n",
    "    \n",
    "    # Buffer dataframe to upload\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index = False)\n",
    "\n",
    "    resp = s3.put_object(Bucket = S3_BUCKET, Key = path_S3, Body = csv_buffer.getvalue())\n",
    "    return resp\n",
    "    \n",
    "def dmatch_to_df(dmatch, batch_num, columns_df):\n",
    "    df = pd.DataFrame(dmatch.items())\n",
    "    df.columns = columns_df\n",
    "    df[\"batch_id\"] = batch_num\n",
    "    return df\n",
    "\n",
    "def dmatch_to_S3(dmatch, batch_num, folder_path):\n",
    "    \n",
    "    # Get the S3 file with that name\n",
    "    name_file = f'{batch_num}.csv'\n",
    "    \n",
    "    # From the match_results\n",
    "    df_S3 = load_df_s3(folder_path, name_file)\n",
    "    \n",
    "    # Get the dmatch and convert it to dataframe\n",
    "    df_match = dmatch_to_df(dmatch, batch_num, columns_df = [\"track_id\",\"url\"])\n",
    "    \n",
    "    # Join the existing df in S3 with the df_match for that iteration\n",
    "    df_concat = pd.concat([df_S3, df_match], axis=0)\n",
    "    \n",
    "    # Save DMATCH to S3\n",
    "    res = save_df_to_S3(df_concat, folder_path, name_file, S3_BUCKET = 'musicemotions')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [2020-03-31 17:23:45] Batch 1              | 3 Starting webscrapping...\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop at iter  1\n",
      "Qeyring query clean list  shuffle montgomery herbie nichols\n",
      "Qeyring track_id  TRNDCYY128F92E87BA\n",
      "Qeyring query  shuffle montgomery herbie nichols\n",
      "query_t_song 1\n",
      "query_t_song 2\n",
      "query_t_song 3\n",
      "query_t_song 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop at iter  2\n",
      "Qeyring query clean list  waiting for a savior metal church\n",
      "Qeyring track_id  TRTNEEE128F930C8E6\n",
      "Qeyring query  waiting for a savior metal church\n",
      "query_t_song 1\n",
      "query_t_song 2\n",
      "query_t_song 3\n",
      "query_t_song 4\n",
      "Uploading  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [2020-03-31 17:23:50] Batch 1              |     3.1 Uploaded Counter Iteration: 2\n",
      "2it [00:04,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop at iter  3\n",
      "Qeyring query clean list  down on the corner kathy mattea\n",
      "Qeyring track_id  TRSBXFQ128E079833F\n",
      "Qeyring query  down on the corner kathy mattea\n",
      "query_t_song 1\n",
      "query_t_song 2\n",
      "query_t_song 3\n",
      "query_t_song 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:05,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop at iter  4\n",
      "Qeyring query clean list  i laugh myself to sleep soda fountain\n",
      "Qeyring track_id  TRSGRBN128F4226061\n",
      "Qeyring query  i laugh myself to sleep soda fountain\n",
      "query_t_song 1\n",
      "query_t_song 2\n",
      "query_t_song 3\n",
      "query_t_song 4\n",
      "Uploading  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [2020-03-31 17:23:53] Batch 1              |     3.1 Uploaded Counter Iteration: 4\n",
      "4it [00:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering loop at iter  5\n",
      "Qeyring query clean list  special woman rabbit\n",
      "Qeyring track_id  TRTQSQC128F92DFC5A\n",
      "Qeyring query  special woman rabbit\n",
      "query_t_song 1\n",
      "query_t_song 2\n",
      "query_t_song 3\n",
      "query_t_song 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:07,  1.50s/it]\n",
      "INFO     [2020-03-31 17:23:54] Batch 1              | 3 Starting webscrapping... (Completed)\n",
      "INFO     [2020-03-31 17:23:54] Batch 1              | 5. Succesfully run batch: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#    WebScrapping - Iterate\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#Log\n",
    "log.info(\"3 Starting webscrapping...\")\n",
    "dict_match = dict()\n",
    "browser = webdriver.Chrome(options=options)\n",
    "counter_iteration = 0\n",
    "\n",
    "iter_download = 100; #upload each 100 iterations\n",
    "\n",
    "for ii, row in tqdm.tqdm(df.iterrows()):\n",
    "    counter_iteration += 1\n",
    "    \n",
    "    track_id = row[\"track_id\"]\n",
    "    query = row[\"query\"]\n",
    "    batch_id = row[\"batch_id\"]\n",
    "    query_clean_set = row[\"query_clean_set\"]\n",
    "    query_clean_list = row[\"query_clean_list\"]\n",
    "    \n",
    "    try:\n",
    "        # Query yt song and compare the titles with the query set\n",
    "        result = query_yt_song(query_clean_list, query_clean_set)\n",
    "    except:\n",
    "        print(\"ERROR \", ii)\n",
    "        continue\n",
    "    \n",
    "    # Save the cases in which we find a match\n",
    "    if len(result): #there is a href\n",
    "        dict_match[track_id] = result\n",
    "\n",
    "    # Upload each 100 iterations\n",
    "    if (counter_iteration % iter_download) == 0: #save each iter_download queried songs\n",
    "        res = dmatch_to_S3(dict_match, batch_num, \"match-results\")        \n",
    "        dict_match = dict()\n",
    "        log.info(f\"    3.1 Uploaded Counter Iteration: {counter_iteration}\")\n",
    "    if ii >5:\n",
    "        break\n",
    "        \n",
    "browser.close()\n",
    "log.info(\"3 Starting webscrapping... (Completed)\")\n",
    "log.info(f\"5. Succesfully run batch: {batch_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the log file to S3\n",
    "def file_to_S3(local_path, S3_path,  S3_BUCKET = 'musicemotions'):\n",
    "    \"\"\"\n",
    "    local_path = os.path.join(\"..\",\"webscrapping\",\"log\",\"WebScrap.log\")\n",
    "    S3_path = nonmatch-query/log.txt\n",
    "    \"\"\"\n",
    "    if S3_path:\n",
    "        s3 = boto3.resource('s3')\n",
    "        resp = s3.Object(S3_BUCKET, S3_path).put(Body=open(local_path, 'rb'))\n",
    "    else:\n",
    "        s3 = boto3.resource('s3')\n",
    "        resp = s3.Object(S3_BUCKET, S3_path).put(Body=open(local_path, 'rb'))\n",
    "    return resp\n",
    "\n",
    "# Upload log file to log/nonmatch-query\n",
    "# path log in S3\n",
    "log_S3_path = f\"log/nonmatch-query/{batch_num}.log\"\n",
    "resp_log = file_to_S3(path_local_log_file, log_S3_path,  S3_BUCKET = 'musicemotions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload the log file to S3\n",
    "# def file_to_S3(local_path, S3_path,  S3_BUCKET = 'musicemotions'):\n",
    "#     \"\"\"\n",
    "#     local_path = os.path.join(\"..\",\"webscrapping\",\"log\",\"WebScrap.log\")\n",
    "#     S3_path = nonmatch-query/log.txt\n",
    "#     \"\"\"\n",
    "#     if S3_path:\n",
    "#         s3 = boto3.resource('s3')\n",
    "#         resp = s3.Object(S3_BUCKET, S3_path).put(Body=open(local_path, 'rb'))\n",
    "#     else:\n",
    "#         s3 = boto3.resource('s3')\n",
    "#         resp = s3.Object(S3_BUCKET, S3_path).put(Body=open(local_path, 'rb'))\n",
    "#     return resp\n",
    "\n",
    "# # Upload log file to log/nonmatch-query\n",
    "# # path log in S3\n",
    "# log_S3_path = f\"log/nonmatch-query/{batch_num}.log\"\n",
    "# resp_log = file_to_S3(path_local_log_file, log_S3_path,  S3_BUCKET = 'musicemotions')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
